# Uncertainty in Neural Networks

This repository contains the official PyTorch implementation of:

**Uncertainty in Neural Relational Inference**  
Candidate Number: 8271V
Supervisor: Pietro Lió 



**Abstract:**  Neural Networks (NNs) have been found to perform well in inferring the interaction graphs and reconstructing the trajectory of interacting particles, a problem applicable to many physical and biological phenomena. One downfall of the implementations to date is that they are not able to predict the uncertainties in their outputs, which would be useful in order to analyse the results, calibrate equipment, allow for more informed engineering and understanding of the system.  
    The evaluation of errors in NNs has recently received a lot of attention and methods investigating obtaining uncertainty by setting random nodes to zero (dropout) or predictions of the errors from outputs of NNs with different structures have been investigated. However, these methods do not allow for propagation of uncertainties across time-steps in Markovian processes such as trajectory reconstruction. In this paper, I use the factorised neural relational inference (fNRI) model.
    To allow for this propagation, the fNRI model was extended to output both a mean and a standard deviation for the position and velocity predictions which, together with an appropriate loss function, can account for uncertainty. A variety of loss functions were investigated including ideas from convexification and a Bayesian treatment of the problem.
    I show that it is necessary to consider the physical meaning of the variables when considering the uncertainty, indicating that a significant component of the uncertainty is physical. I further show that the increased dimensionality allows the NN to find undesirable local minima. I show that it is possible to use convexification to force the model into a more desirable minimum. I also show that complicating the models is detrimental to performance. Finally, I show that the minimum the model reaches depends not only on the loss function but also on the choice of initial conditions. 


Much of the code here is based on https://github.com/ekwebb/fNRI (MIT licence) which in turn is based on https://github.com/ethanfetaya/NRI (MIT licence). The author would like to thank Ezra Webb, Ben Day, Helena Andres-Terre and Pietro Lió for making the codebase for the factorised Neural Relational Inference model (arXiv:1905.08721) publicly available on  https://github.com/ekwebb/fNRI (MIT licence). The author would also like to thank Thomas Kipf, Ethan Fetaya, Kuan-Chieh Wang, Max Welling & Richard Zemel for making the codebase for the Neural Relational Inference model (arXiv:1802.04687) publicly available on https://github.com/ethanfetaya/NRI (MIT licence).

### Requirements
* Pytorch 1.3.1
* Python 3.7

### Data generation

To replicate the experiments on simulated physical data, first generate training, validation and test data by running:

```
cd data
python generate_dataset.py
```
This generates the ideal springs and charges dataset as in the experiments ran in this paper. To add noise change the parameter `--noise_type` to `--noise_type Pink` for pink noise. Change it to `--noise_type Brownian` for Brownian noise. 
Then, generate the data for the computational and physical errors by running:

```
python generate_dataset_comperrors.py
python generate_dataset_physerrors.py
```

### Run experiments

From the project's root folder, run
```
python train_main.py
```
to train an fNRI model on the dataset. To run the standard NRI model, add the `--NRI` argument. You can specify a different dataset by modifying the `sim-folder` argument: `--sim-folder nameofdatasetfile` by default the data set file is springcharge_5 which will be generated by default by the dataset generator. You can train using a different model by changing the argument `--loss_type`: `--loss_type anisotropic` will use the anisotropic model. By default this is set to train using the fixed variance model.

The different models that can be trained are:\
 `--loss_type isotropic` - trains using the fully isotropic model\
 `--loss_type anisotropic` - trains using the full anisotropic model\
 `--loss_type semi_isotropic` - trains using the semi_isotropic model\
 `--loss_type lorentzian` - trains using the Lorentzian model\
 `--loss_type norminvwishart` - trains using the anisotropic Normal-Inverse-Wishart model\
 `--loss_type kalmanfilter` - trains using the full anisotropic model with a kalman filter. This model is based off of an algorithm suggested by: Multivariate Uncertainty in Deep Learning, Rebecca L. Russell, Christopher Reale, 2019, arXiv:1910.14215 \
 `--loss_type KL` - trains using the anisotropic model with the additional KL prior term\
 To plot the data from a model that has been trained, use the `--plot` argument. To load the data from the folder stored add the parameter: `--load-folder name_of_folder_the_data_is_stored`. Then to plot add: `--plot True`
 To change sigma add the parameter `--var value_of_sigma_sqrd`
 To use the 3-layer MLP model or the model with random features modify the argument `--decoder`. For the 3-layer MLP use:  `--decoder mlp3` and for adding random features use `--decoder mlpr`. By default it is set to use the 2-layer MLP model.
 
 To train using the correlations model from the root folder run 
 ```
python train_sigmawcorrelations.py
```
To train using log(σ^2) from the root folder run
```
python train_logsigma.py
```